{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 4: EM, DL & RL\n",
    "### 1. Neural Networks: MNIST image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_test(data, labels, params):\n",
    "    h, output, cost = forward_prop(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def compute_accuracy(output, labels):\n",
    "    accuracy = (np.argmax(output, axis=1) == np.argmax(\n",
    "        labels, axis=1)).sum() * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size), labels.astype(int)] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for input. \n",
    "    Use tricks from previous assignment to avoid overflow\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    s = x / np.sum(np.exp(x))\n",
    "    # END YOUR CODE\n",
    "    return s\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    # END YOUR CODE\n",
    "    return s\n",
    "\n",
    "\n",
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    return hidder layer, output(softmax) layer and loss\n",
    "    \"\"\"\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    '''\n",
    "    data: a n by 1 vector, where n = 784\n",
    "    W1: a k by n matrix, where k = 300 is the number of hidden units\n",
    "    W2: a 10 by k matrix\n",
    "    '''\n",
    "    # 1. Input layer to the hidden layer\n",
    "    h = sigmoid(W1 @ data + b1)\n",
    "\n",
    "    # 2. Hidden layer to output layer\n",
    "    y = softmax(W2 @ h + b2)\n",
    "\n",
    "    # 3. Compute the cost\n",
    "    cost = -np.sum(labels * np.log(y))\n",
    "\n",
    "    # END YOUR CODE\n",
    "    return h, y, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a neural network in a more general form, we define the following notations:  \n",
    "* $X^{L}$: the input of layer L, a n by 1 matrix (vector)  \n",
    "* $W^{L}$: the weight between layer $L$ and layer $L+1$, a m by n matrix (where m is the number of the neurons in the $L+1 $ layer and n is the number of neurons in the $L$ layer)\n",
    "* $Z^{L}=W^{L}X^L$: the output of the $L$ layer, a m by 1 matrix\n",
    "* $A^{L}=f(Z^{L})$: the activation output of the $L$ layer, a m by 1 matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation involves 3 steps:\n",
    "1. Calculate the error term of the output layer directly\n",
    "2. Calculate the error term of all the other layers by using the error term of its later layer (back propagate the error)\n",
    "3. Calculate the derivates using the error term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: the multivariate chain rule**  \n",
    "For function $z=f(x,y)$, where $x,y$ is functions of $t$, therefore we have:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial t} = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the error term as:\n",
    "$$\n",
    "\\delta^{L} = \\frac{\\partial \\ell}{\\partial Z^{L}}\n",
    "$$\n",
    "where $\\ell$ is the loss function. By the definition of gradient, we know that $\\delta^L$ is a K by 1 matrix.\n",
    "For the error term of the output layer, we begin by examing one element in that error term:\n",
    "\\begin{align*}\n",
    "\\delta_i^{L} &= \\frac{\\partial \\ell}{\\partial Z_i^{L}}\\\\\n",
    "&= \\sum_{k}\\frac{\\partial \\ell}{\\partial A_k^{L}} \\frac{\\partial A_k^{L}}{Z_i^{L}}\\\\\n",
    "&= \\frac{\\partial \\ell}{\\partial A_i^L}\\frac{\\partial A_i^L}{Z_i^L}\n",
    "\\end{align*}\n",
    "We take this question as an example, in this problem, the loss function $\\ell$ is the cross-entropy, which has the following form:\n",
    "$$\n",
    "\\ell = -\\sum_{k=1}^Ky_k\\log \\hat{y}_k\n",
    "$$\n",
    "and also $\\hat{y}_k=A_k^L=f(Z_1^L,\\dots,Z_k^L,\\dots, Z_K^L)$ where $f$ is the softmax function. Therefore, the above equation is:\n",
    "\\begin{align*}\n",
    "\\delta_i^{L} &= \\frac{\\partial \\ell}{\\partial A_i^L}\\frac{\\partial A_i^L}{Z_i^L}\\\\\n",
    "&= -\\sum_{k}^K\\frac{y_k}{\\hat{y}_k}\\frac{\\partial \\hat{y}_k}{\\partial Z_i^L}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the softmax function, we have:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\hat{y}_k}{\\partial Z_i^L}=\\left\\{\n",
    "    \\begin{matrix}\n",
    "    \\frac{e^{z_j}\\sum_{k=1}^Ke^{z_k}-e^{2z_j}}{(\\sum_{k=1}^Ke^{z_k})^2}=\\frac{e^{z_j}}{\\sum_{k=1}^Ke^{z_k}}-\\frac{e^{z_j}}{\\sum_{k=1}^Ke^{z_k}}\\frac{e^{z_j}}{\\sum_{k=1}^Ke^{z_k}}={\\hat{y}_k(1-\\hat{y}_k)}&i==k\\\\\n",
    "    -\\frac{e^{z_j+z_k}}{(\\sum_{k=1}^Ke^{z_k})^2}=-\\hat{y}_k\\hat{y}_i&i\\ne k\n",
    "    \\end{matrix}\n",
    "\\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete matrix form is:\n",
    "\\begin{align*}\n",
    "\\delta^L&=\\frac{\\partial \\ell}{\\partial Z^L}\\\\\n",
    "&= J_zA\\nabla_A\\ell\n",
    "\\end{align*}\n",
    "where $J_zA$ is a K by K Jacobian matrix ($J_zA_{ik}=\\frac{\\partial \\hat{y}_k}{\\partial Z_i^L}$) and $\\nabla_A\\ell$ is a K by 1 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the other layers except the output layer, we have the corresponding error term as:\n",
    "\\begin{align*}\n",
    "\\delta_i^{L-1} &= \\frac{\\partial\\ell}{\\partial Z_i^{L-1}}\\\\\n",
    "&= \\sum_{k=1}^K\\frac{\\partial \\ell}{\\partial Z_k^{L}}\\frac{\\partial Z_k^{L}}{\\partial Z_i^{L-1}}\\\\\n",
    "&= \\sum_{k=1}^K\\frac{\\partial \\ell}{\\partial Z_k^{L}}\\frac{\\partial Z_k^{L}}{\\partial A_i^{L-1}}\\frac{\\partial A_i^{L-1}}{\\partial Z_i^{L-1}}\\\\\n",
    "&= \\sum_{k=1}^K\\delta_k^{L}W_{ki}^{L-1}\\sigma(Z_i^{L-1})(1-\\sigma(Z_i^{L-1}))\n",
    "\\end{align*}\n",
    "where $\\sigma(x)$ is the sigmoid function and we use the fact that $\\sigma'=\\sigma(1-\\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete matrix form is:\n",
    "\\begin{align*}\n",
    "\\delta^{L-1}={\\delta^L}^TW^{L-1}\\odot\\bigg(\\sigma(Z^{L-1})(1-\\sigma(Z^{L-1}))\\bigg)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivate can be calculated by:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell}{\\partial W_{ij}^{L-1}}&=\\sum_{k=1}^K\\frac{\\partial \\ell}{\\partial Z_k^L}\\frac{\\partial Z_k^L}{\\partial W_{ij}^{L-1}}\\\\\n",
    "&=\\frac{\\partial \\ell}{\\partial Z_i^L}\\frac{\\partial Z_i^L}{\\partial W_{ij}^{L-1}}\\\\\n",
    "&=\\delta_i^LA_j^{L-1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete matrix form is:\n",
    "\\begin{align*}\n",
    "H_{W^{L-1}}\\ell=\\delta^L{A^{L-1}}^T\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    return gradient of parameters\n",
    "    \"\"\"\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # First, we do a forward propagation\n",
    "    h, y, cost = forward_prop(data, labels, params)\n",
    "\n",
    "    # Second, we calculate all error\n",
    "    nabla_loss = labels / y\n",
    "    nabla_softmax = -y @ y.T\n",
    "    nabla_softmax -= np.diag(np.diag(nabla_softmax))\n",
    "    nabla_softmax += np.diag((y * (1 - y)).reshape(-1))\n",
    "    delta_2 = nabla_softmax @ nabla_loss\n",
    "\n",
    "    # Propagate backward\n",
    "    delta_1 = (delta_2.T @ W2).reshape([-1, 1]) * (h * (1 - h))\n",
    "\n",
    "    # Get the gradient\n",
    "    gradW1 = delta_1.reshape([-1, 1]) @ data.reshape([1, -1])\n",
    "    gradb1 = delta_1\n",
    "    gradW2 = delta_2.reshape([-1, 1]) @ h.reshape([1, -1])\n",
    "    gradb2 = delta_2\n",
    "    # END YOUR CODE\n",
    "\n",
    "    grad = {}\n",
    "    grad['W1'] = gradW1\n",
    "    grad['W2'] = gradW2\n",
    "    grad['b1'] = gradb1\n",
    "    grad['b2'] = gradb2\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def total_error(output, label):\n",
    "    # output is a m by 10 matrix, where m is the number of training examples\n",
    "    return np.sum(label * np.log(output)) / output.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(trainData, trainLabels, devData, devLabels):\n",
    "    print('Training begin')\n",
    "    (m, n) = trainData.shape\n",
    "    num_hidden = 300\n",
    "    learning_rate = 5\n",
    "    params = {}\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Initialize the parameter\n",
    "    params['b1'] = np.zeros([num_hidden,1])\n",
    "    params['b2'] = np.zeros([10,1])\n",
    "    params['W1'] = np.random.randn(num_hidden, n)\n",
    "    params['W2'] = np.random.randn(10, num_hidden)\n",
    "    \n",
    "    B = 1000 # Mini batch size\n",
    "    iteration_per_epoch = m / 1000\n",
    "    epoch_round = 30\n",
    "    error_history = [0] * epoch_round\n",
    "    for i in range(epoch_round):\n",
    "        print('Epoch %d begin' % (i + 1))\n",
    "        for j in range(int(iteration_per_epoch)):\n",
    "            # One batch\n",
    "            batch_data = np.array(trainData[j * B:(j + 1) * B])\n",
    "            batch_label = np.array(trainLabels[j * B:(j + 1) * B])\n",
    "            grad_total = {k:np.zeros(params[k].shape) for k in params}\n",
    "            for data, label in zip(batch_data, batch_label):\n",
    "                grad = backward_prop(data.reshape(-1,1), label.reshape(-1,1), params)\n",
    "                for k in grad:\n",
    "                    grad_total[k] += grad[k]\n",
    "            # Update parameter\n",
    "            for k in params:\n",
    "                params[k] -= (learning_rate / B) * grad_total[k]\n",
    "        # Calculate error averaged over the entire data set\n",
    "        for data, label in zip(trainData, trainLabels):\n",
    "            _, _, loss = forward_prop(data, label, params)\n",
    "            error_history[i] += loss\n",
    "        print('Epoch %d end, error %f' % (i + 1, error_history[i]))\n",
    "    # END YOUR CODE\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading data\n",
      "Loading end\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "print('Begin loading data')\n",
    "trainData, trainLabels = readData(\n",
    "    './data/images_train.csv', './data/labels_train.csv')\n",
    "trainLabels = one_hot_labels(trainLabels)\n",
    "p = np.random.permutation(60000)\n",
    "trainData = trainData[p, :]\n",
    "trainLabels = trainLabels[p, :]\n",
    "\n",
    "devData = trainData[0:10000, :]\n",
    "devLabels = trainLabels[0:10000, :]\n",
    "trainData = trainData[10000:, :]\n",
    "trainLabels = trainLabels[10000:, :]\n",
    "\n",
    "mean = np.mean(trainData)\n",
    "std = np.std(trainData)\n",
    "trainData = (trainData - mean) / std\n",
    "devData = (devData - mean) / std\n",
    "\n",
    "testData, testLabels = readData(\n",
    "    './data/images_test.csv', './data/labels_test.csv')\n",
    "testLabels = one_hot_labels(testLabels)\n",
    "testData = (testData - mean) / std\n",
    "print('Loading end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "Training begin\n",
      "Epoch 1 begin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloud/.local/lib/python3.6/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in log\n",
      "/home/cloud/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: RuntimeWarning: overflow encountered in exp\n",
      "/home/cloud/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  del sys.path[0]\n",
      "/home/cloud/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in subtract\n",
      "/home/cloud/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-457519ed1bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Begin training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreadyForTesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreadyForTesting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b1db9c446376>\u001b[0m in \u001b[0;36mnn_train\u001b[0;34m(trainData, trainLabels, devData, devLabels)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mgrad_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mgrad_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-3b12cc79765c>\u001b[0m in \u001b[0;36mbackward_prop\u001b[0;34m(data, labels, params)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Get the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mgradW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mgradb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mgradW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Begin training')\n",
    "params = nn_train(trainData, trainLabels, devData, devLabels)\n",
    "\n",
    "readyForTesting = False\n",
    "if readyForTesting:\n",
    "        accuracy = nn_test(testData, testLabels, params)\n",
    "        print('Test accuracy: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Out',\n",
       " '_',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i23',\n",
       " '_i24',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'autopep8',\n",
       " 'backward_prop',\n",
       " 'compute_accuracy',\n",
       " 'devData',\n",
       " 'devLabels',\n",
       " 'exit',\n",
       " 'forward_prop',\n",
       " 'gc',\n",
       " 'get_ipython',\n",
       " 'json',\n",
       " 'mean',\n",
       " 'nn_test',\n",
       " 'nn_train',\n",
       " 'np',\n",
       " 'one_hot_labels',\n",
       " 'p',\n",
       " 'plt',\n",
       " 'quit',\n",
       " 'readData',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'std',\n",
       " 'testData',\n",
       " 'testLabels',\n",
       " 'total_error',\n",
       " 'trainData',\n",
       " 'trainLabels']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21241"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del total_error\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

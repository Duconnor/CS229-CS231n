{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 3: Deep Learning & Unsupervised Learning\n",
    "### 4. KL divergence and Maximum Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the definition of KL divergence and Jensen's inequality, $\\forall P,Q$ we have:\n",
    "\\begin{align*}\n",
    "KL(P||Q)&=\\sum_{x}P(x)\\log\\frac{P(x)}{Q(x)}\\\\\n",
    "&=E[-\\log \\bigg(\\frac{Q(x)}{P(x)}\\bigg)]\\\\\n",
    "&\\ge -\\log\\bigg( E[\\frac{Q(x)}{P(x)}]\\bigg)\\\\\n",
    "&= -\\log\\bigg(\\sum_{x}P(x)\\frac{Q(x)}{P(x)}\\bigg)\\\\\n",
    "&=-\\log\\bigg(\\sum_{x}Q(x)\\bigg)\\\\\n",
    "\\end{align*}\n",
    "Note that the first inequality holds since $-\\log x$ is a convex function. And because $\\sum_{x}Q(x)=1$, therefore we have:\n",
    "\\begin{align*}\n",
    "KL(P||Q)&\\ge-\\log\\bigg(\\sum_{x}Q(x)\\bigg)\\\\\n",
    "&=-\\log1=0\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$:  \n",
    "Because $-\\log x$ is strictly convex and $KL(P||Q)=0$ implies that:\n",
    "$$E[-\\log \\bigg(\\frac{Q(x)}{P(x)}\\bigg)]= -\\log\\bigg( E[\\frac{Q(x)}{P(x)}]\\bigg)$$\n",
    "By using the result of Jensen's inequality, we know that $\\frac{Q(x)}{P(x)}=c$, where $c$ is a constant. Because $P$ and $Q$ are distribution themselves, therefore $c$ could only be $1$ or otherwise, either $P$ or $Q$ can't satisfy the constraint that $\\sum_xP(x)=1$. So we have $P=Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Leftarrow$:  \n",
    "If $P=Q$, then we have $\\frac{Q(x)}{P(x)}=1$ is a constant. Therefore, by using the result of Jensen's inequality, we know that:\n",
    "$$\n",
    "KL(P||Q)=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\\begin{align*}\n",
    "&KL(P(X)||Q(X))+KL(P(Y|X)||Q(Y|X))\\\\\n",
    "&=\\sum_{x}P(x)\\log\\frac{P(x)}{Q(x)}+\\sum_{x}P(x)\\bigg(\\sum_{y}P(y|x)\\log\\frac{P(y|x)}{Q(y|x)}\\bigg)\\\\\n",
    "&=\\sum_{x}P(x)\\log\\frac{P(x)}{Q(x)}+\\sum_{x}\\sum_{y}P(y,x)\\log\\frac{P(y|x)}{Q(y|x)}\\\\\n",
    "&=\\sum_{x}P(x)\\log\\frac{P(x)}{Q(x)}+\\sum_{x}\\bigg(\\sum_{y}P(y,x)\\log\\frac{P(y,x)}{Q(y,x)}+\\sum_{y}P(y,x)\\log\\frac{Q(x)}{P(x)}\\bigg)\\\\\n",
    "&=\\sum_{x}P(x)\\log\\frac{P(x)}{Q(x)}+\\sum_{x}\\bigg(\\sum_{y}P(y,x)\\log\\frac{P(y,x)}{Q(y,x)}+P(x)\\log\\frac{Q(x)}{P(x)}\\bigg)\\\\\n",
    "&=\\sum_{x}P(x)\\log\\frac{P(x)}{Q(x)}+\\sum_{x}\\sum_{y}P(y,x)\\log\\frac{P(y,x)}{Q(y,x)}+\\sum_xP(x)\\log\\frac{Q(x)}{P(x)}\\\\\n",
    "&=\\sum_{x}\\sum_{y}P(y,x)\\log\\frac{P(y,x)}{Q(y,x)}+\\sum_xP(x)\\log 1\\\\\n",
    "&=KL(P(X,Y)||Q(X,Y))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the definition of KL divergence, we have:\n",
    "\\begin{align*}\n",
    "KL(\\hat{P}||P_\\theta)=\\sum_x\\hat{P}(x)\\log\\frac{\\hat{P}(x)}{P_\\theta(x)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, minimizing $KL(\\hat{P}||P_\\theta)$ is the same as maximizing $\\log P_\\theta$, which is:\n",
    "$$\n",
    "\\arg\\min_\\theta KL(\\hat{P}||P_\\theta)=\\arg\\max_\\theta \\sum_{i=1}^m\\log P_\\theta(x^{(i)})\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 3: Deep Learning & Unsupervised Learning\n",
    "### 2. EM for MAP estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixtutres of Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning**  \n",
    "Every latent random variable $z^{(i)}$ ($i$ indicates that this is the latent random variable for training example $x^{(i)}$) obey the same multinomial distribution that has $k$ different values ($z^{(i)}\\sim Multinomial(\\phi)$). We model the training data by assuming each training example was generated by randomly choosing $z^{(i)}$ from $\\{1,2,3,\\dots,k\\}$ and then $x^{(i)}$ was drawn from the corresponding Gaussian distribution specifying by $z^{(i)}$. Therefore, the log likelihood of the training data can be written as:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta)&=\\sum_{i=1}^{m}\\log p(x^{(i)};\\theta)\\\\\n",
    "&=\\sum_{i=1}^{m}\\log \\sum_{z^{(i)}=1}^{k}p(x^{(i),z^{(i)}};\\theta)\n",
    "\\end{align*}\n",
    "Note that the above $p(x^{(i),z^{(i)}};\\theta)$ can be obtained by simply substituting $x^{(i)}$ into the probability expression of the $z^{(i)}$'th Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM algorithm is an effective optimization method. By applying Jensen's inequality to our log likelihood function, we can get different lower bounds for **different set of $\\theta$**. The EM algorithm involves two main strategies: E-step and M-step. For a given objective function $\\ell(\\theta)$, in the E-step, we construct and tight (make sure that $\\ell(\\theta^{t}) \\ge \\ell(\\theta^{t+1})$) a lower-bound for $\\ell(\\theta)$, and in the M-step, we try to optimize that lower-bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the E-step, we choose the proper set of values of $Q_i(z^{(i)})$, where $i$ corresponds to the i'th example, and for each training example, we have a laten variable $z^{(i)}$ corresponding to it, while in the M-step, we choose the proper value of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the EM algorithm to optimize the above mixture of Gaussian problem, in the E-step, we have:\n",
    "\\begin{align*}\n",
    "Q_i(z^{(i)} = j) &= p(z^{(i)}=j|x^{(i)};\\theta)=\\frac{p(z^{(i)} = j, x^{(i)};\\theta)}{\\sum_{l=1}^{k}p(z^{(i)} = l,x^{(i)};\\theta)}\\\\\n",
    "&=\\frac{p(x^{(i)}|z^{(i)}=j;\\theta)p(z^{(i)}=j)}{\\sum_{l=1}^{k}p(x^{(i)}|z^{(i)}=l;\\theta)p(z^{(i)}=l)}\n",
    "\\end{align*}\n",
    "and in the M-step, we have:\n",
    "\\begin{align*}\n",
    "\\theta = \\arg\\max_\\theta\\ell(\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM algorithm is very similar to the optimization algorithm we used in k-means clustering problem. The choice of the value of $Q_i(z^{(i)})$ is the 'soft' guess of value of $z^{(i)}$ (which correspond to the centroid in k-means clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized for MAP Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have the likelihood function as:\n",
    "$$\n",
    "\\bigg(\\prod_{i=1}^m\\sum_{z^{(i)}}p(x^{(i)},z^{(i)}|\\theta)\\bigg)p(\\theta)\n",
    "$$\n",
    "The log likelihood is:\n",
    "$$\n",
    "\\ell(\\theta) = \\sum_{i=1}^m\\log \\sum_{z^{(i)}}p(x^{(i)},z^{(i)}|\\theta)+\\log p(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the E-Step, we try to construct a lower bound for $\\ell(\\theta)$ by choosing a proper set of values for $Q_i$.  \n",
    "For each i, let $Q_i$ be the distribution over the $z^{(i)}$'s, then we have:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta) &= \\sum_{i=1}^m \\log \\sum_{z^{(i)}}Q_i(z^{(i)}) \\frac{p(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})} + \\log p(\\theta)\\\\\n",
    "&\\ge \\sum_{i=1}^m \\sum_{z^{(i)}} Q_i(z^{(i)}) \\log \\frac{p(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})} + \\log p(\\theta)\n",
    "\\end{align*}\n",
    "The above inequality comes from the fact that $\\log$ function is concave and Jensen's inequality. For Jensen's inequality to hold with equality, we have:\n",
    "$$\n",
    "\\frac{p(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})}=c\n",
    "$$\n",
    "where $c$ is some constant. Thus, we have:\n",
    "$$\n",
    "Q_i(z^{(i)}) = c\\times p(x^{(i)},z^{(i)}|\\theta)\n",
    "$$\n",
    "Because we have the fact that $\\sum_{z^{(i)}}=1$, we have:\n",
    "\\begin{align*}\n",
    "c\\times \\sum_{z^{(i)}}p(x^{(i)},z^{(i)}|\\theta)=1\\Rightarrow\n",
    "c = \\frac{1}{\\sum_{z^{(i)}}p(x^{(i)},z^{(i)}|\\theta)}\n",
    "\\end{align*}\n",
    "Therefore, by applying Bayes' rule, we have:\n",
    "\\begin{align*}\n",
    "Q_i(z^{(i)})&=\\frac{ p(x^{(i)},z^{(i)}|\\theta)}{\\sum_{z^{(i)}}p(x^{(i)},z^{(i)}|\\theta)}\\\\\n",
    "&=p(z^{(i)}|x^{(i)};\\theta)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### M-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the M-Step, our goal is trying to maximize the lower bound we just construct by choosing a proper value for parameter $\\theta$.  \n",
    "Since now we have:\n",
    "$$\n",
    "\\ell(\\theta) = \\sum_{i=1}^m \\sum_{z^{(i)}} Q_i(z^{(i)})\\log \\frac{p(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})}+\\log p(\\theta)\n",
    "$$\n",
    "Therefore, we need to set:\n",
    "$$\n",
    "\\theta = \\arg\\max_{\\theta} \\sum_{i=1}^m \\sum_{z^{(i)}} Q_i(z^{(i)})\\log \\frac{p(x^{(i)},z^{(i)}|\\theta)}{Q_i(z^{(i)})}+\\log p(\\theta)\n",
    "$$\n",
    "The above is tractable because we have the assumption that both $\\log p(x,z|\\theta)$ and $\\log p(\\theta)$ are concave in $\\theta$. Therefore, the above equation only requires maximizing a linear combination of these two quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proof of Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prove by showing that $\\prod_{i=1}^mp(x^{(i)}|\\theta)p(\\theta)$ monotonically increases with each iteration of the above algorithm.  \n",
    "We have:\n",
    "\\begin{align*}\n",
    "\\log \\prod_{i=1}^mp(x^{(i)}|\\theta^{k+1})p(\\theta^{k+1})&\\ge \\sum_{i=1}^m \\sum_{z^{(i)}} Q_i^{k}(z^{(i)}) \\log \\frac{p(x^{(i)},z^{(i)}|\\theta^{k+1})}{Q_i^{k}(z^{(i)})} + \\log p(\\theta^{k+1})\\\\\n",
    "&\\ge \\sum_{i=1}^m \\sum_{z^{(i)}} Q_i^k(z^{(i)}) \\log \\frac{p(x^{(i)},z^{(i)}|\\theta^{k})}{Q_i^k(z^{(i)})} + \\log p(\\theta^{k})\\\\\n",
    "&= \\log \\prod_{i=1}^mp(x^{(i)}|\\theta^{k})p(\\theta^{k})\n",
    "\\end{align*}\n",
    "The above first inequality comes from the fact that $\\sum_{i=1}^m \\sum_{z^{(i)}} Q_i^{k}(z^{(i)}) \\log \\frac{p(x^{(i)},z^{(i)}|\\theta)}{Q_i^{k}(z^{(i)})} + \\log p(\\theta)$ is a lower bound for $\\log \\prod_{i=1}^mp(x^{(i)}|\\theta^{k+1})p(\\theta^{k+1})$. The second inequality comes from the fact that in the M-Step, we try to maximize the lower bound, therefore, the new result must be greater or equal to the previous result. And the last equality comes from the fact that in the E-Step, we choose a proper set of value (i.e. $Q_i^k$) to tight the lower bound."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

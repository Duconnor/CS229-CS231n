{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 2: Supervised Learning II\n",
    "### 2. Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, our optimization goal is maximizing the log likelihood $\\ell(\\theta)$, which is defined as follows:\n",
    "\\begin{align*}\n",
    "\\ell(\\theta)=\\sum_{i=1}^my\\log h_\\theta(x^{(i)})+ (1-y)\\log(1-h_\\theta(x^{(i)}))\n",
    "\\end{align*}\n",
    "where $h_\\theta(x^{(i)}) = \\frac{1}{1-e^{-\\theta^{\\mathrm T}x^{(i)}}}$. By using just one training example, we can calculate its derivative as:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta_i}\\ell(\\theta) = (y-h_{\\theta}(x))x_j\n",
    "\\end{align*}\n",
    "So our $\\theta$ should let $\\ell(\\theta)=0$.  \n",
    "Then, if we take all training examples into consideration, we now have:\n",
    "\\begin{align*}\n",
    "X^{\\mathrm T}(Y-h_\\theta(X))=\\left[\n",
    "\\begin{matrix}\n",
    "\\sum_{i=1}^m(y^{(i)}-h_{\\theta}(x^{(i)}))\\\\\n",
    "\\sum_{i=1}^m(y^{(i)}-h_{\\theta}(x^{(i)}))x^{(i)}_1\\\\\n",
    "\\vdots\\\\\n",
    "\\sum_{i=1}^m(y^{(i)}-h_{\\theta}(x^{(i)}))x^{(i)}_n\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\end{align*}\n",
    "Note that we have that first row because we add an extra bias term in our data matrix $X$  \n",
    "Considering only the first row, we have:\n",
    "\\begin{align*}\n",
    "&\\sum_{i=1}^m(y^{(i)}-h_{\\theta}(x^{(i)}))=0\\\\\n",
    "&\\sum_{i=1}^mh_\\theta(x^{(i)})=\\sum_{i=1}^my^{(i)}\\\\\n",
    "&\\frac{\\sum_{i\\in I_{0,1}} P(y^{(i)}=1|x^{(i)};\\theta)}{|i\\in I_{0,1}|}=\\frac{\\sum_{i\\in I_{0,1}}{\\bf {1}}\\{y^{(i)}=1\\}}{|i\\in I_{0,1}|}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model is prefectly calibrated, then it means that its output has a good probabilistic interpretation, which does not necessarily imply perfect accuracy.  \n",
    "On the other hand, if a model has perfect accuracy, it means it can always predict right. So it surely has good probabilistic Interpretation on its output, i.e. it is perfecyly calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229, Fall 2017\n",
    "## Problem Set 2: Supervised Learning II\n",
    "### 5. Kernelizing the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.**  \n",
    "Refer to [What the Hell is Perceptron?](https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $\\theta^{(0)}=\\bf 0$ and the update rule is:\n",
    "$$\\theta^{(i+1)}=\\theta^{(i)}+\\alpha{\\bf 1}\\{h_{\\theta^{(i)}}(x^{i+1})y^{(i+1)}<0\\}y^{(i+1)}x^{(i+1)}$$\n",
    "We can write $\\theta^{(i)}$ is the form of linear combination of previous training examples:\n",
    "\\begin{align*}\n",
    "\\theta^{(i)}={\\bf 0}+\\sum_{n=1}^i\\alpha{\\bf 1}\\{h_{\\theta^{(n)}}(\\phi(x^{n+1}))y^{(n+1)}<0\\}y^{(n+1)}\\phi(x^{(n+1)})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $h_{\\theta^{(i)}}(x^{(i+1)})$, we have:\n",
    "\\begin{align*}\n",
    "h_{\\theta^{(i)}}(x^{(i+1)})&={\\mathrm{sign}}({\\theta^{(i)}}^{\\mathrm T}\\phi(x^{(i+1)}))\\\\\n",
    "&={\\mathrm{sign}}\\bigg((\\sum_{n=1}^i\\alpha{\\bf 1}\\{h_{\\theta^{(n)}}(\\phi(x^{n+1}))y^{(n+1)}<0\\}y^{(n+1)}\\phi(x^{(n+1)}))^{\\mathrm T}\\phi(x^{(i+1)})\\bigg)\\\\\n",
    "&={\\mathrm{sign}}\\bigg(\\sum_{n=1}^i\\bigg(\\alpha{\\bf 1}\\{h_{\\theta^{(n)}}(\\phi(x^{n+1}))y^{(n+1)}<0\\}y^{(n+1)}\\phi(x^{(n+1)})^{\\mathrm T}\\bigg)\\phi(x^{(i+1)})\\bigg)\\\\\n",
    "&={\\mathrm{sign}}\\bigg(\\sum_{n=1}^i\\bigg(\\alpha{\\bf 1}\\{h_{\\theta^{(n)}}(\\phi(x^{n+1}))y^{(n+1)}<0\\}y^{(n+1)}K(x^{(n+1)},x^{(i+1)})\\bigg)\\bigg)\n",
    "\\end{align*}\n",
    "where $K$ is the Mercer kernel corresponding to feature mapping $\\phi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new update rule will be:\n",
    "\\begin{align*}\n",
    "\\theta^{(i+1)}&=\\theta^{(i)}+\\alpha{\\bf 1}\\{h_{\\theta^{(n)}}(\\phi(x^{n+1}))y^{(n+1)}<0\\}y^{(n+1)}\\phi(x^{(n+1)})\\\\\n",
    "&=\\sum_{n=1}^{i+1}\\alpha{\\bf 1}\\{h_{\\theta^{(n)}}(\\phi(x^{n+1}))y^{(n+1)}<0\\}y^{(n+1)}\\phi(x^{(n+1)})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: it is okay if we can't calculate $\\theta$ explicitly, we just need to remember the predicted result of every seen example, and then when calculating the new result, what we need is only these results and $K(x^{(n+1)},x^{(i+1)})$, for every $n\\le i$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To finish this question, I refer to [this wikipedia page](https://en.wikipedia.org/wiki/Kernel_perceptron).*  \n",
    "This question teaches us, if we want to apply 'kernel trick' to some other learning algorithms other than the SVM, it is important to be able to write the prediction function in the form of inner product between **input vector** and some/all **training vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
